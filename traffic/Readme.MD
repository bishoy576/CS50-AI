This convolutional neural network  consists of three sets of convolutional layers followed by max pooling and dropout layers for regularization. The first convolutional layer (`Conv2D`) has 32 filters of size 3x3, with 'same' padding to preserve the input size, applied to an input image of size 28x28 with 3 channels (RGB). ReLU activation functions (`Activation('relu')`) follow each convolutional layer to introduce non-linearity. A max pooling layer (`MaxPooling2D`) with a pool size of 2x2 reduces spatial dimensions by half, and a dropout layer (`Dropout(0.25)`) randomly sets 25% of the inputs to zero during training to prevent overfitting. The second set of convolutional layers repeats this structure with 64 filters in each layer. After another max pooling and dropout layer, the flattened output is fed into a fully connected (`Dense`) layer with 392 neurons, activated by ReLU. A dropout layer further reduces overfitting by randomly dropping 50% of the inputs. Finally, the output layer consists of 43 neurons (corresponding to 43 classes in the dataset) with a softmax activation function, which outputs probabilities for each class. The RMSprop optimizer with a learning rate of 0.0001 and a decay rate of 1e-6 is used to minimize the categorical cross-entropy loss during training, and the model is evaluated based on accuracy metrics.

This architecture is adapted from Aditya Mehrotra's great article: [Creating a CNN using Keras for GTSRB](https://medium.com/analytics-vidhya/creating-a-cnn-using-keras-for-gtsrb-c67e647fb9ca)

I experimented with the same architecture provided in the lecture's source code for classifying handrwriiten digits but I ended up with a 6% accuracy. I then Tried adding 2 more sequences of convolution and pooling layers. each with a kernel size of 4 x 4 but I didn't see much improvement.  
